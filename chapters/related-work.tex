\chapter{Related work and literature review}\label{chp:relatedWorkAndLitReview}

\epigraph{Learn from the mistakes of others. You can't live long enough to make them all yourself.}{Eleanor Roosevelt}
%\epigraph{The whole is greater than the sum of its parts.}{Aristotle}

This chapter surveys several related works and doses a literature review. It contains a selection of
works that are typically related to applying \acrshortpl{llm} specifically or \acrshort{ml} more
generally to \acrshort{ads} simulator scenarios.

\section{Literature review}\label{sec:literatureReview}

This section surveys the current state of the research field with a theoretical
perspective. Applied pieces of work are saved for later, to be surveyed in the
latter part of the chapter.

% NOTE: Store deler av denne seksjonen er AI slop fra UiO GPT (jeg har caved på mine policies)
\subsection{Survey of LLM applications in scenario-based ADS testing}

\citeauthor{surveyLLMScenarioBasedTesting} provide an extensive overview of the
various ways that \acrshortpl{llm} have been applied to scenario-based testing
of \acrlongpl{ads}. The authors classify existing research primarily by testing
phase within a scenario-based pipeline and, within those phases, by the
functional roles assigned to
\acrshortpl{llm}~\cite{surveyLLMScenarioBasedTesting}. Their survey is
continually updated; the last update was made two months before the time of
writing.\footnote{As of September 17th 2025, the last update to their
    \href{https://github.com/ftgTUGraz/LLM4ADSTest}{GitHub repository} was on July
    23rd, 2025. The paper on arXiv was last updated May 22nd, 2025.} This
necessarily entails some overlap with works we review in the later section of applied works --
\Cref{sec:relatedWork}.

Not deterred by this, let us delve into the survey. They start by highlighting
the trend between the number of \acrshort{llm} surveys and \acrshort{ads}
surveys. While both trends increased from 2020--2023, there was an explosion in
\num{2024}, with around \num{200} works concerning applications of
\acrshortpl{llm} for \acrlong{ads} purposes being published~\cite[p.~1,
    Fig.~(b)]{surveyLLMScenarioBasedTesting}. Furthermore, the number of
\acrshort{ads} studies has remained comparatively steady over the last \num{4}
years, whereas the number of \acrshort{llm} studies has exploded in
popularity~\cite[p.~1, Fig.~(a)]{surveyLLMScenarioBasedTesting}. This suggests
that a significant share of the scientific effort around \acrshortpl{ads} in the
last year has been concerned with leveraging \acrshortpl{llm}.
% Comment: Drawing such interpretive links is acceptable when supported by clear
% trends in cited figures; we take care to phrase this as a suggestion rather
% than a definitive causal claim.

\subsubsection{Meta-survey review}

The article summarizes the field by synthesizing surveys from four related
subfields:
\begin{inparaenum}
    \item general \acrshort{llm} surveys,
    \item surveys of scenario-based testing,
    \item surveys on \acrshortpl{llm} for \acrshortpl{ads}, and
    \item surveys on \acrshortpl{llm} applied to miscellaneous domains
\end{inparaenum}, highlighting their respective
foci~\cite[p.~2]{surveyLLMScenarioBasedTesting}.
% Note: The plural of "focus" is indeed "foci".

At a high level, the meta-review emphasizes three converging threads: (i) rapid
maturation of \acrshortpl{llm} (including multimodal variants) and prompting
strategies; (ii) consolidation of scenario-based testing concepts (scenario
abstraction levels, coverage, safety assessment); and (iii) a surge of
cross-domain techniques (e.g., RAG, knowledge graphs, alignment) being
repurposed for \acrshort{ads} development and validation.

\subsubsection{The categories of ways of applying LLMs for ADS testing}

Rather than proposing isolated, disjoint categories,
\citeauthor{surveyLLMScenarioBasedTesting} organize applications by testing
phase in a canonical scenario-based workflow, and within phases, by the
functional role played by the \acrshort{llm}. The phases and roles can be
summarized as follows~\cite{surveyLLMScenarioBasedTesting}:

\paragraph{Phase I: Scenario Source}
Here, \acrshortpl{llm} are used to augment, curate, and retrieve data:
\begin{itemize}
    \item Data enrichment: synthesizing trajectories, future driving videos, and
          editable scenes from language; coupling \acrshortpl{llm} with
          diffusion/world models to expand scarce corner cases.
    \item Hazard-driven enrichment: partially automating HARA and STPA to seed
          hazardous scenarios; \acrshortpl{llm} assist experts but require human
          validation.
    \item Data labelling: auto-annotation (e.g., JSON labels) for benchmarks;
          typically followed by manual quality checks.
    \item Data retrieval: natural-language queries over video/image logs via
          VLM/MLLM pipelines, vector DBs, and BEV-aware retrieval.
\end{itemize}

\paragraph{Phase II: Scenario Generation}
This is the most active area and features five functional roles for
\acrshortpl{llm} within the generation pipeline:
\begin{enumerate}
    \item LLM as human–machine interface: translating user intent (natural
          language) into structured information, loss functions, or executable code
          that downstream generators (e.g., diffusion/transformers) can consume.
    \item LLM as data interpreter: extracting structured knowledge from accident
          reports, standards, technical docs, or naturalistic logs (including BEV
          maps) and turning them into scenario elements.
    \item LLM as intermediate-format generator: producing intermediate
          representations (e.g., driving policies, scenario elements,
          functional/abstract/logical scenarios) that a later stage will compile.
    \item LLM as standardized-format generator: compiling intermediate forms
          into standardized scenario formats (e.g., \texttt{OpenSCENARIO},
          \texttt{AVUnit}) ready for simulators.
    \item LLM as executable scenario generator: directly generating
          simulator-executable files via (a) template filling, (b) end-to-end
          text/multimodal-to-code, or (c) hybrid approaches combining retrieval,
          templating, and compiler/simulator feedback.
\end{enumerate}

\paragraph{Phase III: Scenario Selection}
\acrshortpl{llm} are used for realism assessment to filter implausible scenarios
and prioritize test cases that better match real-world distributions.

\paragraph{Phase IV: Test Execution}
\acrshortpl{llm} support:
\begin{itemize}
    \item Anomaly detection: reasoning over perception outputs to flag semantic
          anomalies.
    \item Simulation setup automation: translating language to simulator
          configurations and enabling interactive modification.
    \item Scenario optimization: iterative code/syntax repair and fidelity
          refinement using simulator/compiler feedback loops.
\end{itemize}

\paragraph{Phase V: ADS Assessment}
\acrshortpl{llm} assist in generating simulation reports, legal/causal analysis
of incidents, and early explorations of intelligence-level evaluation (e.g.,
\acrshort{cot} with \acrshort{rag} and human-in-the-loop validation).

This phase-and-role taxonomy is particularly useful because it aligns research
contributions with concrete points of integration in existing validation
pipelines, clarifying where \acrshortpl{llm} can add value and what auxiliary
tools (retrievers, simulators, compilers) are typically required.

\subsubsection{The \num{5} key challenges when applying LLMs for ADS testing}

\citeauthor{surveyLLMScenarioBasedTesting} identify five open challenges and
research directions~\cite{surveyLLMScenarioBasedTesting}:

\paragraph{1) Hallucination and output variability}
\acrshortpl{llm} may generate factually incorrect content or inconsistent
outputs for identical prompts. In \acrshort{ads} testing this manifests as
physically implausible trajectories, invalid scenario code, or unreliable
legal/safety judgments. Mitigations include strict format constraints,
verification agents, retrieval-augmented prompting, and
simulator/compiler-in-the-loop validation—yet robust guarantees remain open.

\paragraph{2) Simulation platform integration}
Heterogeneous scenario formats (e.g., XOSC, SCENIC, SUMO XML), differing
simulator APIs, and limited end-to-end automation hinder portability. Promising
directions are standardized intermediate representations, stronger
schema/ontology constraints during generation, and tighter runtime feedback
loops for automatic refinement.

\paragraph{3) Lack of domain-specific models}
Most reported systems rely on prompt engineering of general-purpose
\acrshortpl{llm}. Insufficient domain adaptation leads to syntax/semantics
errors, misinterpretation of standards, and weak temporal/causal grounding.
Combining targeted fine-tuning (or parameter-efficient adaptation) with
\acrshort{rag} over authoritative \acrshort{ads} corpora is indicated.

\paragraph{4) Insufficient attention to scenario databases}
Despite heavy investment in generation, there is little work on storing,
deduplicating, validating, and reusing scenarios at scale. \acrshortpl{llm}
could assist with syntax/physics checks, semantic clustering, coverage
accounting, and regulation-aware tagging to turn ad hoc outputs into
institutional memory.

\paragraph{5) Industrial application gap}
Industry uptake is early-stage, concentrated on a few tasks (e.g., XOSC
generation, test automation). Barriers include data privacy/security,
operational costs of on-prem model hosting, explainability/traceability
(ISO~21448), and integration maturity. Bridging this gap will likely require
hybrid systems (private \acrshortpl{llm} + curated knowledge bases), rigorous
audit trails, and certifiable interfaces.


\subsection{LLM4AD}

LLM4AD is a survey paper by \citeauthor{LLM4AD} that gives a broad overview of
applications of \acrshortpl{llm} for \acrlong{ads} purposes. It touches on
several of the various \acrshort{ads} applications where \acrshortpl{llm} are
relevant such as
\begin{inparaenum}
    \item language interaction,
    \item contextual understanding,
    \item zero-shot and few-shot planning allowing \acrshortpl{llm} to perform
    tasks they were not trained on, helping with handling edge cases,
    \item continuous learning and personalization, and
    \item interpretability and trust \end{inparaenum}~\cite[p.~2]{LLM4AD}.
Furthermore, the authors propose a comprehensive benchmark for evaluating the
instruction-following abilities of an \acrshort{llm}-based system in
\acrshort{ads} simulation~\cite[p.~1]{LLM4AD}.

\subsection{Synthesis}

Whereas LLM4AD surveys the breadth of \acrshort{llm} use across \acrshort{ads}
(including perception, planning, interaction, and evaluation),
\citeauthor{surveyLLMScenarioBasedTesting} narrow the lens to scenario-based
testing and provide a phase-aligned taxonomy with concrete functional roles.
Together, they paint a consistent picture: \acrshortpl{llm} are most mature in
generation-centric workflows and human-in-the-loop tooling, with pressing needs
in standardization, traceability, and domain-grounded reasoning for
safety-critical adoption.

\section{Related work}\label{sec:relatedWork}

Having obtained an overview of the current state of the literature, we proceed to surveying several
pieces of applied works. Here, they are categorized broadly with regard to what they do and how the
do it.

There is some overlap between some of the works and several of the categories. A work gets allocated
to the category in which that fits the best with regard to the \emph{focus} of the contribution of the work.

\subsection{ADS scenario generation}

The following works relate to generating \acrshort{ads} test scenarios using traditional
\acrshort{ml} techniques.

\subsubsection{Dataset and toolset -- DeepScenario}\label{sec:deepScenario}

DeepScenario is both a dataset and a toolset aimed at \acrlong{ads} testing~\cite{DeepScenario}. The
principal value proposition of this work lies in recognizing the fact that \begin{inparaenum}
    \item there are an infinite number of possible driving scenarios, and
    \item generating critical driving scenarios is very costly with regard to time costs and
    computational resources\end{inparaenum}~\cite[52]{DeepScenario}. The authors therefore propose
an open driving scenario of more than \num{30000} driving scenarios focusing on \acrshort{ads}
testing~\cite[52]{DeepScenario}. The project utilises traditional machine learning
methodologies, having been performed prior to the broad adaptation of \acrshortpl{llm}.

Its scenarios are intended for the simulator SVL by LG (\Cref{sec:simulatorOverview}).

\subsubsection{Test case specification language -- RTCM}

RTCM is a \acrshort{ads} testing framework that allows the user to utilise natural language for
synthesizing test cases. The authors propose a domain-specific language --- called RTCM, after
\textsc{Restricted Test Case Modelling} --- for specifying test cases. It is based on natural language
and composed of \begin{inparaenum}
    \item an easy-to-use template,
    \item a set of restriction rules, and
    \item keywords \end{inparaenum}~\cite[397]{RTCM}.  Furthermore, they also propose a tool to
take this RTCM source code as input and generating either \begin{inparaenum}
    \item manual, or
    \item automatically \end{inparaenum} executable test cases~\cite[397]{RTCM}. The proposed tools
were evaluated in experiments with industry partners, successfully generating executable test
cases~\cite[397]{RTCM}.

\subsubsection{Generating crash scenarios -- DeepCollision}

\citeauthor{deepCollision} utilise \acrfull{rl} for \acrshort{ads} testing, with the goal of getting
the \acrshort{ads} to \textit{collide}. They used \textit{collision probability} for the loss
function of the \acrlong{rl} algorithm~\cite[384]{deepCollision}. Their experiments included
training 4 DeepCollision models, then using \begin{inparaenum}
    \item random, and
    \item greedy
\end{inparaenum} models for generating a baseline to compare their models with. The results showed
that DeepCollision demonstrated significantly better effectiveness in obtaining collisions than the
baselines. While not specifically focused on \textit{testing}, we recognize that their work is
thematically similar to our project.

\subsection{Utilising LLMs on ADS scenarios}

The remaining works all relate to utilising \acrshortpl{llm} for various purposes related to
\acrshort{ads} scenarios.

\subsubsection{AutoSceneGen}\label{sec:autoSceneGen}

AutoSceneGen is a framework for \acrshort{ads} testing using \acrshortpl{llm},
focusing on the motion planning of \acrlong{ads}~\cite[14539]{autoSceneGen}.
\citeauthor{autoSceneGen} highlights how \acrshortpl{llm} provide opportunities
for efficiently evaluating \acrshort{ads} in a cost-effective
manner~\cite[14539-14540]{autoSceneGen}. They generate a substantial set of synthetic scenarios and
experiment with using \begin{inparaenum}
    \item only synthetic data,
    \item only real-world data, and
    \item a combination of the \num{2} \end{inparaenum} as training data. They find that motion
planners trained with their synthetic data significantly outperforms those trained solely on
real-world data~\cite[14539]{autoSceneGen}.

\subsubsection{LLM-Driven testing of ADS}%\acrshort{ads}}

\citeauthor{LLMDrivenTestingADS24} worked on using \acrshortpl{llm} to for automated test generation
based on free-form textual descriptions in the area of automotive~\cite[173]{LLMDrivenTestingADS24}.
They propose a prototype for this purpose and evaluate their proposal for \acrshort{ads} driving
feature scenarios in Carla. They used the \acrshortpl{llm} GPT-4 and Llama3, finding GPT-4 to
outperform Llama3 for the stated purpose. Their findings include this \acrshort{llm}-powered test
methodology to be more than \num{10} times faster than traditional methodologies while reducing
cognitive load~\cite[173]{LLMDrivenTestingADS24}.
% TODO: Cognitive load -> brain atrophy (sec:llMproblems)

\subsubsection{Requirements All You Need?}

\citeauthor{requirementsAllYouNeed} provide an overview of \acrshortpl{llm} for \acrshort{ads} in
their recent preprint~\citetitle{requirementsAllYouNeed}\footnote{This was submitted to Arxiv on
    2025-05-19.}, focusing on \acrshort{llm}'s abilities for translating abstract requirements extracted
from automotive standards and documents into configuration for Carla (\Cref{sec:simulatorOverview})
simulations~\cite{requirementsAllYouNeed}. Their experiments include employing the
\textit{autonomous emergency braking} system and the sensors of the \acrshort{ads}. Furthermore, they
split the requirements into \num{3} categories: \begin{inparaenum}
    \item vehicle descriptions,
    \item test case pre-conditions, and
    \item test case post-conditions (\Nref{sec:testingConditions})
\end{inparaenum}~\cite{requirementsAllYouNeed}. The preconditions they used included
\begin{inparaenum}
    \item agent placement,
    \item desired agent behaviour, and
    \item weather conditions amongst others\end{inparaenum}, whereas their postconditions reflected
the desired outcomes of the tests, primarily related to the vehicle's
telemetry~\cite{requirementsAllYouNeed}.

\subsubsection{Language Conditioned Traffic Generation}

\citeauthor{languageconditionedtrafficgeneration} look into using \acrshortpl{llm} to generate
specific traffic scenarios. They identify the importance of being able to use simulators to test
\acrshortpl{ads}, and highlight how test scenarios are expensive to
obtain~\cite[1]{languageconditionedtrafficgeneration}. To this end, they propose a tool --
\textsc{LTCGen} which employs the strengths of \acrshortpl{llm} to match a natural language query
with a fitting underlying map\footnote{Map as in a \textit{world} in which a scenario can take
    place.}, and populates this with a \begin{inparaenum}
    \item initial traffic distribution, and
    \item the dynamics of all the vehicles involved in the scene.
\end{inparaenum}
Something to note is that they generate their scenarios, without initially taking the \textit{ego
    vehicle} into account. The ego vehicle of the scene is simply determined as the vehicle that is
in the \textit{centre} of the first
\textit{frame}~\cite[3]{languageconditionedtrafficgeneration}.

\subsubsection{Scenario engineer GPT}

\citeauthor{seGpt} outline a framework for utilising the \acrshort{llm}-backed ChatGPT in order to
generate scenarios. They propose SeGPT -- a scenario generation framework that they found to yield
\textit{significant progress in the domain of scenario generation}~\cite[4422]{seGpt}. They posit
that their prompt engineering ensures that the generated scenarios are authentically diverse and
challenging~\cite[4423]{seGpt}. The focus is primarily on \textit{trajectory
    scenarios}~\cite[4422-4423]{seGpt}.

% TODO: Dette avsittet virker litt malplassert - det virker mer som discussion enn RW
Note how they explicitly mention scenario \textit{generation}. Our approach for this project has a
different angle, with the focus being on modifying \textit{existing} scenarios. More on this in
\Nref{chp:solutionProposal}. The difference between generating a `brand new' scenario with a model
trained on existing scenarios, and modifying an existing scenario seems like a matter of
granularity. These are very similar concepts, only that the enhanced scenario will have more common
DNA whereas the other `new' scenario will consist of a broader range of DNA from its various
underlying scenario corpora.

\subsubsection{LLM driven scenario generation}\label{sec:rwChang24}

\citeauthor{LLMScenarioChang24} also look into using \acrlongpl{llm} to generate \acrshort{ads}
scenarios. They recognize several of the challenges we outline in \Cref{sec:problemDescription}. In
their \citeyear{LLMScenarioChang24} paper, they propose \textsc{LLMScenario}, which is an
\acrshort{llm}-backed framework for both \begin{inparaenum}
    \item scenario generation, and
    \item  evaluation feedback tuning
\end{inparaenum}~\cite[6581]{LLMScenarioChang24}.

They analyse scenarios in order to provide the \acrshort{llm} with a minimum baseline scenario
description, and propose score functions based on both \begin{inparaenum}
    \item reality and
    \item rarity.
\end{inparaenum} Their prompting is based on \acrfull{cot} and a posteriori empirical experience.
Lastly, they tested several \acrlongpl{llm} for their experiments. Their results were positive,
indicating effectiveness for scenario engineering in Industry 5.0~\cite[6581]{LLMScenarioChang24}.
% TODO: Burde finne ut av hva in nomine christi "industry 5.0" er så jeg ikke risikerer å skrive noe
% som ikke gir mening.

\subsubsection{Chat2Scenario}

\citeauthor{chat2Scenario} propose a method for utilising \acrshortpl{llm} to retrieve
\acrshort{ads} scenarios given a natural language query. Their framework synthesizes scenarios from
naturalistic\footnote{Their term. The intended meaning of \textit{naturalistic} is not all clear to
    me.} driving datasets, based on observation real world human driving~\cite[55]{chat2Scenario}, that
it then uses as a database for retrieving the scenario that best matches the user's natural
language query. Furthermore, they employ traditional techniques for asserting the relevance of the
retrieved scenarios, allowing the user to specify a set of \textit{criticality metrics}, of which a
certain threshold must be reached amongst the scenarios that are initially retried by the
\acrshort{llm}, pruning false positives. As a measure to increase the usability of their framework,
they also provide a web-app with an intuitive \acrshort{gui} for both \begin{inparaenum}
    \item operating the tool, and
    \item visualizing the scenarios \end{inparaenum}~\cite[560]{chat2Scenario}.

In order to allow the \acrshort{llm} to determine whether a scenario is relevant under the
provided query, they put forward a method for classifying the various scenarios using traditional
\acrshort{ml} techniques. This classification focuses primarily on highway scenarios and the
activities of other actors in relation to the ego vehicle~\cite[561-562]{chat2Scenario}.

% \textbf{Prompt engineering} <- det ser nesten helt likt ut som et nytt work...

% TODO: Legge inn kryssreferanse til background om prompt engineering? 

The project's prompts are `informed' by the \num{6} \acrlong{oai} guidelines from their prompt
engineering guide\footnote{\url{https://platform.openai.com/docs/guides/prompt-engineering} (URL
    from the paper.)}, ending up with a structured prompt of \num{5} segments. These segments serve to
guide the \acrshort{llm}, delineating its role as an `advanced \acrshort{ai} tool for scenario
analysis, specifically tasked with interpreting driving scenario following a pre-established
classification model'~\cite[562]{chat2Scenario}. They then input the user-provided description of
the scenario they wish to retrieve. Following this, a third segment declares the format for the
\acrshort{llm} response, followed by a prime example of \acrlong{icl}, demonstrating what a
satisfactory fulfilment of the desired format could look like. Lastly they instruct the
\acrshort{llm} to \textit{Remember to analyse carefully and provide the classification as per the
    structure given above}~\cite[563]{chat2Scenario}.
% TODO: Ref siste punkt om "husk å gjøre det riktig" -> kan skrive om dette fenomenet i background
% og så referere tilbake til det.

\subsubsection{Predicting driving comfort in autonomous vehicles using road information and multi-
    head attention models}

The \citeyear{Chen2025} article of \citeauthor{Chen2025}~\cite{Chen2025}, delves into the various
aspects related to predicting driving comfort in autonomous vehicles based on \begin{inparaenum}
    \item available road information, and
    \item multi-head attention models.
\end{inparaenum}
Their principal focus is on driving \emph{comfort}. To this end, they evaluate \acrshortpl{ads} in
light of the \textbf{jerk} metric in various situations. Furthermore, they highlight how a high
complexity in the scenarios can increase the probability of emergency breaking occurring, which is
naturally antithetical to comfort for the \acrshort{ads} operator and their passengers.

In order to measure this comfort, they rely metrics calculated from data-points from the
\acrshort{ads} system -- jerk and acceleration. This, they use in conjunction with manual human
driving evaluation scores, to compose a new metric, the `driving comfort evaluation score'
(DCES)~\cite[10]{Chen2025}.

Moreover, they use this information to propose a model -- the \acrfull{adcp} model -- for
\emph{predicting} driving comfort from road information~\cite[2]{Chen2025}.