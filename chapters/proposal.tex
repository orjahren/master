\chapter{\hefe~implementation}\label{chp:solutionProposal}

\epigraph{The only difference between a problem and a solution is that people understand the solution.}{Charles F. Kettering}

Recall that have seen in the \Nref{chp:introduction} and \Nref{chp:background}
that there are several complexities involved with \acrshort{ads} testing, such
that we typically use simulator-based testing to aim at verifying the safety of
the \acrshort{ads} before it gets deployed to the real world. But then we saw
further that simulator-based testing can lead to a false sense of safety due to
the \acrshort{ads} passing all our test scenarios, and then there can be a hole
in what they test for, so that the \acrshort{ads} may actually have
undiscovered faults that we haven't caught on to.

Futhermore, we saw that \acrshortpl{llm} have capabilities for modifying textual
data (such as e.g. scenario definitions) by natural language prompt engineering.
In \Nref{chp:relatedWorkAndLitReview} we surveyed several projects that aim to
enhance scenario-based testing of \acrshortpl{ads}, and we saw that some of them
employed \acrshortpl{llm}. But no extant research has specifically used
\acrshortpl{llm} to \emph{decrease driveability} of \acrshortpl{ads}
scenarios\footnote{The most similar work, to the author's knowledge,
is~\cite{yao2025agentsllm} -- \citetitle{yao2025agentsllm}.}. 

This research gap is addressed by this thesis -- we propose a novel
\acrshort{llm}-powered methodology for decreasing the driveability of
\acrshort{ads} scenarios so that when the \acrshort{ads} is tested with the less
driveable test scenarios, we can either \begin{inparaenum}
    \item trigger it to fail and analyse what went wrong, or 
    \item be more confident in the \acrshort{ads} being able to operate in
    complex scenarios due to passing them.
\end{inparaenum}

To this end, we propose \hefe\footnote{`'\acrshortpl{llm} for decreased driveability'.}~-- a tool for
\begin{inparaenum}
    \item running a base \acrshort{ads} test case,
    \item enhancing the test case using \acrshortpl{llm},
    \item running the enhanced test case, and
    \item comparing the results of the two runs.
\end{inparaenum}
We summarize the motivation behind using the \hefe~tool in the following user story:

\begin{enumerate}
    \item I have a set of \acrshort{ads} test scenarios. I provide this
            set to \hefe. It will run the entire set, and generate a
            baseline of my \acrshort{ads} performance.

    \item \hefe~will then improve my test scenarios using
        \acrshortpl{llm} to make them less driveable,  and run the
        enhanced versions.

    \item Lastly \hefe~will report how the results differ from running
        the base and enhanced version of a test case.

    \item This will give me insight into my \acrshort{ads} by reviewing
        what scenarios it failed to complete Then I can look into the
        cause of the error state and uncover underlying faults in the
        \acrshort{ads} that i did not know about beforehand. If the
        \acrshort{ads} \emph{is} able to complete the less driveable
        scenarios, I can be more confident in it and be more assured
            that it will work properly during real-world operation.
\end{enumerate}

The tool follows a natural pipeline structure. We have some base test scenarios that
need to be run in order to get a baseline for the results, we then have to
enhance these, and run the improved versions and compare them to their original
versions.
Experimenting with this tool will allow us to learn the extent to which
\acrshortpl{llm} can be applied for decreasing  driveability in \acrlong{ads}
test scenarios, satisfying the \Nref{sec:problemDescription}. The tool is
implemented in an \acrshort{llm}- and scenario-agnostic fashion so that it is
scalable to other combinations of \acrshort{llm} and scenario formats than those
experimented with in this specific work to verify the feasibility of the tool.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{experiment-material/accident-pics/base/underway.png}
    \caption{A screenshot from executing a Carla scenario.}\label{fig:carlaScenarioExample}
\end{figure}

\Cref{fig:carlaScenarioExample} renders an example of how it can appear when exeucitng a test
scenario on the Carla simulator. Runs like this will later ber presented in \Nref{chp:results} and
then analysed in \Nref{chp:discussion} to evaluate the value of the \hefe~tool.

\section{\hefe~architectural overview}

In order to decrease the driveability of the \acrshort{ads} scenarios, we need \num{3}
separate components: \begin{inparaenum}
    \item something to handle the \acrshort{llm} interfacing, 
    \item something that can integrate with the \acrshort{ads} simulator, and
    finally 
    \item some kind of human-facing interface to administrate the process. 
\end{inparaenum}
By compartmentalizing what component has responsibility for what task, we reduce
complexity and increase the possibility of repurposing the modules for other
possible tasks in the future. UNIX philosophy!
These components and their relationship is rendered in \Cref{fig:hefeArch}.

For the sake of making their roles more clear, we christen the components as
follows: \begin{inparaenum}
    \item Odin will be the module for interfacing with \acrshortpl{llm} and
    performing the enhancement.
    \item Thor will take \acrshort{ads} scenarios, run them on a simulator and
    report the results. Finally,
    \item Loki will interface with the human and start the process by
    determining what \acrshort{llm} is to be used with what prompts with what scenarios.
\end{inparaenum}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/d2-pdf/hefe.pdf}
    \caption{\hefe~pipeline architecture}\label{fig:hefeArch}
\end{figure}

All the prompts exists as a part of the Odin module. Loki will request available
prompts from Odin on behalf of the user, and the user will select which one they
want to use. Similarly, all the scenarios exist within the Thor module and are
handled by it. As for the prompt, the user reqquests a list of available
scenarios from the Thor module and makes a decision on which they want to use. 

The programming language \textsc{Python} is widely used for \acrshort{ads} simulation. It is a high
level language, allowing the user great flexibility and developer experience.
For this reason, it is the de facto goto-language for \acrshort{ads} simulation
purposes and \hefe~is also implemented using Python.

All code is available on the GitHub repo
\href{https://github.com/orjahren/LLM4DD}{\hefe}.


\section{Component details}

Let us now focus on the individual components and analyse how they solve their
given tasks and how they all fit together to make up the total \hefe~pipeline tool.

\subsection{LLM interface and prompt applications -- Odin}\label{sec:odinImplementation}

% Rest API - performing LLM enhancement - Odin
% “Fast API”, Python
% Take a base test case as body
% Have some prompt repository
% Apply prompts with LLMs
% Must integrate with LLM. Either locally (Ollama) or remote (some API)
% Look into good LLM agnostic transition layer. E.g. Aisuite
% https://github.com/andrewyng/aisuite
% Should use same UUIDs as outlined above, but suffixed with e.g. “pure” and “tainted”
% Containerized. Docker compose?
The Odin module handles all things \acrshort{llm}. It provides a unified
\acrshort{api} for applying various prompts to scenarios and returning the
enhanced output resulting from having applied the prompt. We have implemented
support for the \acrshortpl{llm} that are available on \begin{inparaenum}
    \item Ollama, and
    \item Gemini
\end{inparaenum}. This allows for testing with \acrshortpl{llm} such as
\begin{inparaenum}\setcounter{enumi}{2}
    \item Mistral \num{7.2}B, and
    \item gemini-2.5-flash
\end{inparaenum}.

Note that we don't employ any traditional \acrfull{nlp} techniques related to e.g. tokenization
or input processing -- we leave this up to the internal mechanisms of the
\acrshortpl{llm}.

In order to facilitate testing various \acrshortpl{llm}, the module is written to
be as general as possible, exposing a simple \acrshort{api} that can be used
with all various \acrshortpl{llm} with minimal modiciation necessary.
This allows for experimenting with several \acrshortpl{llm} that may all
have different internal \acrfullpl{api} without having to modify the major underlying code
for specific \acrshortpl{api}. This saves time, and prevents repetetive manual work.

\subsection{LLM interface implementations}

Interfacing with \acrshortpl{llm} is the principal task of the Odin module. It
offers a simple \acrshort{api} to Loki, and then Odin itself will handle all the
complexities of making the \acrshort{llm} work with the provided prompt etc.

As mentioned above, the current version of the \hefe~tool has support for Ollama
and Gemini \acrshortpl{api}. Let us review how this has been achieved:

\subsubsection{Gemini integration}

The Gemini integration is quite straightforward, relying on Google's own
\texttt{genai} Python module. 
The one piece of complexity to not is that it requires that the user provides
their own Gemini \acrshort{api} key and has this set as an environment variable
with the proper name. Without this being as it should, the script has been
written to hard crash, as it would not possible for it to complete the desired
\acrshort{llm} enhancement regardless as long as the \acrshort{api} key is not
present.

% \lstinputlisting[caption={llm\_api\_interfaces/gemini\_interface.py, The implementation of a Gemini interface for executing prompts.}, label={lst:thorGeminiInterface}, language={Python}]{hefe-listings/gemini.py}

\subsubsection{Ollama integration}

The Ollama integration is a bit more cumbersome. This mostly comes down to it not
using any existing library modules for this specific purpose, instead relying on
using the \texttt{json} and \texttt{requests} modules to implement the desired
functionality from scratch, making it so that we need to handle network IO and
marshalling the \acrfull{llm} response into a fitting return buffer.

Its complexity arises principally from \num{2} major factors --
\begin{inparaenum}
    \item the already mentioned manual networking, and
    \item having to parse the streamed response
\end{inparaenum}
Furthermore, this code expects that the user already \emph{has} an Ollama
installation running on their host machine. The code provides no means of setup
for this -- that is an entirely external endeavour that is left up to the end user.

Similarly to how the Gemini implementation does it, this will crash if Ollama is
not functioning properly as it would not possible for it to complete the desired
\acrshort{llm} enhancement regardless if Ollama is unreachable.

% \lstinputlisting[caption={llm\_api\_interfaces/ollama.py, The implementation of an Ollama interface for executing prompts.}, label={lst:thorOllamaInterface}, language={Python}]{hefe-listings/ollama.py}

\subsection{Prompts and their associated code}

In this project, the prompts are the instruction to the \acrfull{llm} for
applying the enhancement to the scenario. Quite possibly the most critical piece
of code related to the experiments. They need to take the base scenario as an
input and integrate it into the \acrshort{llm} context, such that it knows what
it shall use as its base to apply enhancements that will decrease the
driveability. For this reason, it also provides certain scenario
utilities\footnote{That architecturally might as well have been integrated in the
    Thor module\ldots}.
% TODO: Finnes det noe bigbrain måte å slippe å manuelt måtte referere ti llinjenummer?
As mentioned, the prompts need to include the scenarios \emph{in} them, so that
they are accessible to the \acrshort{llm}. How this is done, is rendered in
listing~\ref{lst:odinPromptTestbed}. The most interesting aspect is how the
prompts are stored in the system as lambda functions. This makes it so that they
can take an argument that represents the scenario --
\texttt{python\_carla\_scenario\_raw}(line \#16) -- and simply \emph{execute} the
function to insert the scenario into the prompt (line \#42). This is then
inserted into the output prompt at the location located at line \#21 in the listing.

% TODO: Linjenummere må oppdateres
% TODO: Må legge til at man også kan passe inn scenario-navn og metrics

%\lstinputlisting[caption={experiments/testbed/prompts.py, The implementation of a prompt testbed for executing prompts.}, label={lst:odinPromptTestbed}, language={Python}]{hefe-listings/testbed_prompts.py}

%Lastly, note the comments in the top of the file, intended to give GitHub
%Copilot increased understanding of the context, so that it can provide better
%aid during programming.

%\subsubsection{Scenario utilities}

%These are essentially quite trivial helpers. Listing \ref{lst:odinScenarioUtils}
%renders the core functionality -- hopefully this is quite self-explaining.

%\lstinputlisting[caption={scenario\_utils.py, The implementation of various scenario helper functions for executing prompts.}, label={lst:odinScenarioUtils}, language={Python}]{hefe-listings/scenario_utils.py}

\subsection{Carla interface and scenario utilities -- Thor}\label{sec:testCaseRunner}
% Rest API - running test cases - Thor
% “Fast API”, Python
% POST a test case to the API. It will be ran on the ‘server’
% RabbitMQ for listening for finished test cases? So that the client knows it can fetch the results?
% Will need UUID for test cases so the correct result can be fetched after it has been ran
% Need to store these somewhere. NoSQL database?
% This component should also accumulate results.
% Huge TODO: What metric are these results?
% Should be containerised (Docker/Podman)

The Thor module is responsible for all things related to the Carla \acrshort{ads}
simulator. It provides the client with several scenario-related utilities, and
is capable of executing the desired scenarios.

For the reasons we have seen in~\Cref{sec:simulatorOverview}, we want to run our
scenarios on Carla. It is the best offering as it is open source, under active
development and has a feature rich Python \acrshort{api} with a first party
`scenario runner' module.

Executing \acrshort{ads} scenarios on Carla is quite straightforward when using
Carla's existing Scenario Runner module's functionality. We have made
our own fork of this in order to make it behave in a way that makes the most
sense for the \hefe~situation\footnote{This fork is available on
\url{https://github.com/orjahren/scenario_runner-LLM4DD}. It was mostly used for
debugging purposes and the \hefe~tool should work just fine with the stock
version as well.}. 

We record a plethora of datapoints when executing scenarios on the simulator\footnote{This is again
    provided by the Carla software suite. A complete overview of data points is provided in the
    documentation, see e.g.~\url{https://carla.readthedocs.io/en/0.9.15/adv_recorder/}}.
The way the Carla simulator works, one simulator run can be analysed several
times post factum. The entire scenario execution is stored in a Carla-specific
binary format. This binary file can then later be analysed, extracting various
metrics from one run. This saves time not having to run the simulator more than
necessary, and allows for reproducing the metric calculations from the original
underlying binary log file.

Certain of the Thor utilities are simple tools for asserting the liveness of
Carla, such as the \texttt{get\_carla\_is\_up} function. This function will use
the Carla standard Python library and attempt to connect to the server on its
default port, i.e. \num{2000}, as per the Caral documentation. Note that we
refer to the host as simply \texttt{carla} -- this is possible due to the entire
project running containerised with Docker Compose. Instead of referring to the
specific IP address of the Carla server (typically localhost, if not running it
externally), the Docker system will facilitate this name translation for us.

The Thor healthcheck is used both to assert the general liveness of the pipeline
of the \hefe~pipeline, and to verify that the simulator is available before
performing experiments. By `liveness' we mean to check that all components are running
and ready to process work. Essentially checking that they haven't crashed.
It is better to detect this illegal crash state \emph{before} running experiments rather
than during their execution as that would cause \begin{inparaenum}
    \item cumbersome debugging, and 
    \item wasting time.
\end{inparaenum}

As described in the \Nref{chp:introduction}, we use the \emph{jerk} metric as a
proxy for ride quality and safety. It is implemented as an extension to the
Carla Scenario runner software suite. Using this, we can compare the results
from running the baseline unmodified test case and comparing it with the results
from running the \acrshort{llm}-enhanced version and returning to the user with
regard to driveability. Jerk is to be calculated \emph{after} having executed
the scenario, utilising the Carla binary file described above.

\subsection{Execution tool / user oriented frontend -- Loki}

% Client - orchestrating the process - Loki
% Fetch available test cases from Thor? Select what/which are to be used
% Store results clientside? Separate database for this?

The final module of the \hefe~pipeline is Loki -- it is simply a tool intended
to be used by the user for operating the process. It \begin{inparaenum}
    \item  says what scenarios are available to it (i.e. those that are eligible for
    being enhanced), and
    \item allows the user to select a prompt and
    \item execute that prompt to the scenario of their choosing.
\end{inparaenum}

% \lstinputlisting[caption={loki/main.py, The implementation of the Loki script.}, label={lst:lokiMainImplementation}, language={Python}]{hefe-listings/loki_main.py}

The Loki module relies on the Odin and Thor modules for all essential functionality,
which is in line with what is to be expected as this is simply a frontend client
to \emph{reach} them. For these reasons, it's a quite simple Pytohn script that relies on
\acrshort{rpc} to the \acrshortpl{api} of the other components of the pipeline.