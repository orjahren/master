\chapter{Literature review}\label{chp:literatureReview}

\epigraph{The whole is greater than the sum of its parts.}{Aristotle}

This chapter surveys the current state of the research field with a theoretical
perspective. Applied pieces of work are saved for later, to be surveyed in the
\Nref{chp:relatedWork} chapter. 

% NOTE: Store deler av denne seksjonen er AI slop fra UiO GPT (jeg har caved på mine policies)
\section{Graz University of Technology survey on LLM applications for ADSs}

\citeauthor{surveyLLMScenarioBasedTesting} provide an extensive overview of the
various ways that \acrshortpl{llm} have been applied to scenario-based testing
of \acrlongpl{ads}. The authors classify existing research primarily by testing
phase within a scenario-based pipeline and, within those phases, by the
functional roles assigned to
\acrshortpl{llm}~\cite{surveyLLMScenarioBasedTesting}. Their survey is
continually updated; the last update was made two months before the time of
writing.\footnote{As of September 17th 2025, the last update to their
\href{https://github.com/ftgTUGraz/LLM4ADSTest}{GitHub repository} was on July
23rd, 2025. The paper on arXiv was last updated May 22nd, 2025.} This
necessarily entails some overlap with works we review in \Nref{chp:relatedWork}.

Not deterred by this, let us delve into the survey. They start by highlighting
the trend between the number of \acrshort{llm} surveys and \acrshort{ads}
surveys. While both trends increased from 2020--2023, there was an explosion in
\num{2024}, with around \num{200} works concerning applications of
\acrshortpl{llm} for \acrlong{ads} purposes being published~\cite[p.~1,
Fig.~(b)]{surveyLLMScenarioBasedTesting}. Furthermore, the number of
\acrshort{ads} studies has remained comparatively steady over the last \num{4}
years, whereas the number of \acrshort{llm} studies has exploded in
popularity~\cite[p.~1, Fig.~(a)]{surveyLLMScenarioBasedTesting}. This suggests
that a significant share of the scientific effort around \acrshortpl{ads} in the
last year has been concerned with leveraging \acrshortpl{llm}.
% Comment: Drawing such interpretive links is acceptable when supported by clear
% trends in cited figures; we take care to phrase this as a suggestion rather
% than a definitive causal claim.

\subsection{Meta-survey review}

The article summarizes the field by synthesizing surveys from four related
subfields:
\begin{inparaenum}
    \item general \acrshort{llm} surveys,
    \item surveys of scenario-based testing,
    \item surveys on \acrshortpl{llm} for \acrshortpl{ads}, and
    \item surveys on \acrshortpl{llm} applied to miscellaneous domains
\end{inparaenum}, highlighting their respective
foci~\cite[p.~2]{surveyLLMScenarioBasedTesting}.
% Note: The plural of "focus" is indeed "foci".

At a high level, the meta-review emphasizes three converging threads: (i) rapid
maturation of \acrshortpl{llm} (including multimodal variants) and prompting
strategies; (ii) consolidation of scenario-based testing concepts (scenario
abstraction levels, coverage, safety assessment); and (iii) a surge of
cross-domain techniques (e.g., RAG, knowledge graphs, alignment) being
repurposed for \acrshort{ads} development and validation.

\subsection{The categories of ways of applying LLMs for ADS testing}

Rather than proposing isolated, disjoint categories,
\citeauthor{surveyLLMScenarioBasedTesting} organize applications by testing
phase in a canonical scenario-based workflow, and within phases, by the
functional role played by the \acrshort{llm}. The phases and roles can be
summarized as follows~\cite{surveyLLMScenarioBasedTesting}:

\paragraph{Phase I: Scenario Source}
Here, \acrshortpl{llm} are used to augment, curate, and retrieve data:
\begin{itemize}
    \item Data enrichment: synthesizing trajectories, future driving videos, and
    editable scenes from language; coupling \acrshortpl{llm} with
    diffusion/world models to expand scarce corner cases.
    \item Hazard-driven enrichment: partially automating HARA and STPA to seed
    hazardous scenarios; \acrshortpl{llm} assist experts but require human
    validation.
    \item Data labelling: auto-annotation (e.g., JSON labels) for benchmarks;
    typically followed by manual quality checks.
    \item Data retrieval: natural-language queries over video/image logs via
    VLM/MLLM pipelines, vector DBs, and BEV-aware retrieval.
\end{itemize}

\paragraph{Phase II: Scenario Generation}
This is the most active area and features five functional roles for
\acrshortpl{llm} within the generation pipeline:
\begin{enumerate}
    \item LLM as human–machine interface: translating user intent (natural
    language) into structured information, loss functions, or executable code
    that downstream generators (e.g., diffusion/transformers) can consume.
    \item LLM as data interpreter: extracting structured knowledge from accident
    reports, standards, technical docs, or naturalistic logs (including BEV
    maps) and turning them into scenario elements.
    \item LLM as intermediate-format generator: producing intermediate
    representations (e.g., driving policies, scenario elements,
    functional/abstract/logical scenarios) that a later stage will compile.
    \item LLM as standardized-format generator: compiling intermediate forms
    into standardized scenario formats (e.g., \texttt{OpenSCENARIO},
    \texttt{AVUnit}) ready for simulators.
    \item LLM as executable scenario generator: directly generating
    simulator-executable files via (a) template filling, (b) end-to-end
    text/multimodal-to-code, or (c) hybrid approaches combining retrieval,
    templating, and compiler/simulator feedback.
\end{enumerate}

\paragraph{Phase III: Scenario Selection}
\acrshortpl{llm} are used for realism assessment to filter implausible scenarios
and prioritize test cases that better match real-world distributions.

\paragraph{Phase IV: Test Execution}
\acrshortpl{llm} support:
\begin{itemize}
    \item Anomaly detection: reasoning over perception outputs to flag semantic
    anomalies.
    \item Simulation setup automation: translating language to simulator
    configurations and enabling interactive modification.
    \item Scenario optimization: iterative code/syntax repair and fidelity
    refinement using simulator/compiler feedback loops.
\end{itemize}

\paragraph{Phase V: ADS Assessment}
\acrshortpl{llm} assist in generating simulation reports, legal/causal analysis
of incidents, and early explorations of intelligence-level evaluation (e.g.,
\acrshort{cot} with \acrshort{rag} and human-in-the-loop validation).

This phase-and-role taxonomy is particularly useful because it aligns research
contributions with concrete points of integration in existing validation
pipelines, clarifying where \acrshortpl{llm} can add value and what auxiliary
tools (retrievers, simulators, compilers) are typically required.

\subsection{The \num{5} key challenges when applying LLMs for ADS testing}

\citeauthor{surveyLLMScenarioBasedTesting} identify five open challenges and
research directions~\cite{surveyLLMScenarioBasedTesting}:

\paragraph{1) Hallucination and output variability}
\acrshortpl{llm} may generate factually incorrect content or inconsistent
outputs for identical prompts. In \acrshort{ads} testing this manifests as
physically implausible trajectories, invalid scenario code, or unreliable
legal/safety judgments. Mitigations include strict format constraints,
verification agents, retrieval-augmented prompting, and
simulator/compiler-in-the-loop validation—yet robust guarantees remain open.

\paragraph{2) Simulation platform integration}
Heterogeneous scenario formats (e.g., XOSC, SCENIC, SUMO XML), differing
simulator APIs, and limited end-to-end automation hinder portability. Promising
directions are standardized intermediate representations, stronger
schema/ontology constraints during generation, and tighter runtime feedback
loops for automatic refinement.

\paragraph{3) Lack of domain-specific models}
Most reported systems rely on prompt engineering of general-purpose
\acrshortpl{llm}. Insufficient domain adaptation leads to syntax/semantics
errors, misinterpretation of standards, and weak temporal/causal grounding.
Combining targeted fine-tuning (or parameter-efficient adaptation) with
\acrshort{rag} over authoritative \acrshort{ads} corpora is indicated.

\paragraph{4) Insufficient attention to scenario databases}
Despite heavy investment in generation, there is little work on storing,
deduplicating, validating, and reusing scenarios at scale. \acrshortpl{llm}
could assist with syntax/physics checks, semantic clustering, coverage
accounting, and regulation-aware tagging to turn ad hoc outputs into
institutional memory.

\paragraph{5) Industrial application gap}
Industry uptake is early-stage, concentrated on a few tasks (e.g., XOSC
generation, test automation). Barriers include data privacy/security,
operational costs of on-prem model hosting, explainability/traceability
(ISO~21448), and integration maturity. Bridging this gap will likely require
hybrid systems (private \acrshortpl{llm} + curated knowledge bases), rigorous
audit trails, and certifiable interfaces.

\section{LLM4AD}

LLM4AD is a survey paper by \citeauthor{LLM4AD} that gives a broad overview of
applications of \acrshortpl{llm} for \acrlong{ads} purposes. It touches on
several of the various \acrshort{ads} applications where \acrshortpl{llm} are
relevant such as
\begin{inparaenum}
    \item language interaction,
    \item contextual understanding,
    \item zero-shot and few-shot planning allowing \acrshortpl{llm} to perform
    tasks they were not trained on, helping with handling edge cases,
    \item continuous learning and personalization, and
    \item interpretability and trust \end{inparaenum}~\cite[p.~2]{LLM4AD}.
Furthermore, the authors propose a comprehensive benchmark for evaluating the
instruction-following abilities of an \acrshort{llm}-based system in
\acrshort{ads} simulation~\cite[p.~1]{LLM4AD}.

\section{Synthesis}

Whereas LLM4AD surveys the breadth of \acrshort{llm} use across \acrshort{ads}
(including perception, planning, interaction, and evaluation),
\citeauthor{surveyLLMScenarioBasedTesting} narrow the lens to scenario-based
testing and provide a phase-aligned taxonomy with concrete functional roles.
Together, they paint a consistent picture: \acrshortpl{llm} are most mature in
generation-centric workflows and human-in-the-loop tooling, with pressing needs
in standardization, traceability, and domain-grounded reasoning for
safety-critical adoption.
