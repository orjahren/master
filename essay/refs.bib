@inproceedings{Carla,
  title     = { {CARLA}: {An} Open Urban Driving Simulator},
  author    = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  pages     = {1--16},
  year      = {2017}
}

@article{DeepScenario,
  author  = {Chengjie Lu and Tao Yue and Shaukat Ali},
  title   = {DeepScenario: An Open Driving Scenario Dataset for Autonomous Driving System Testing},
  journal = {IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)},
  year    = {2023},
  pages   = {52--56}
}

@inproceedings{RTCM,
  author    = {Yue, Tao and Ali, Shaukat and Zhang, Man},
  title     = {RTCM: a natural language based, automated, and practical test case generation framework},
  year      = {2015},
  isbn      = {9781450336208},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2771783.2771799},
  doi       = {10.1145/2771783.2771799},
  abstract  = {Based on our experience of collaborating with industry, we observed that test case generation usually relies on test case specifications (TCSs), commonly written in natural language, specifying test cases of a System Under Test at a high level of abstraction. In practice, TCSs are commonly used by test engineers as reference documents to perform these activities: 1) Manually executing test cases in TCSs; 2) Manually coding test cases in a test scripting language for automated test case execution. In the latter case, the gap between TCSs and executable test cases has to be filled by test engineers, requiring a significant amount of coding effort and domain knowledge. Motivated by the above observations from the industry, we first propose, in this paper, a TCS language, named as Restricted Test Case Modeling (RTCM), which is based on natural language and composed of an easy-to-use template, a set of restriction rules and keywords. Second, we propose a test case generation tool (aToucan4Test), which takes TCSs in RTCM as input and generates either manual test cases or automatically executable test cases, based on various coverage criteria defined on RTCM. To assess the applicability of RTCM, we manually modeled two industrial case studies and examined 30 automatically generated TCSs. To evaluate aToucan4Test, we modeled three subsystems of a Video Conferencing System developed by Cisco Systems, Norway and automatically generated executable test cases. These test cases were successfully executed on two commercial software versions. In the paper, we also discuss our experience of applying RTCM and aToucan4Test in an industrial context and compare our approach with other model-based testing methodologies.},
  booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
  pages     = {397–408},
  numpages  = {12},
  keywords  = {Test Case Specification, Test Case Generation, RUCM, Model},
  location  = {Baltimore, MD, USA},
  series    = {ISSTA 2015}
}

@dataset{CriticalScenarios,
  author    = {Crespo Rodriguez, Victor and
               Neelofar and
               Aleti, Aldeida},
  title     = {{Instance Space Analysis of Testing of Autonomous 
               Vehicles in Critical Scenarios}},
  month     = may,
  year      = 2024,
  publisher = {Zenodo},
  version   = 1,
  doi       = {10.5281/zenodo.11202385},
  url       = {https://doi.org/10.5281/zenodo.11202385}
}
@article{luo2024semievol,
  title   = {SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation},
  author  = {Luo, Junyu and Luo, Xiao and Chen, Xiusi and Xiao, Zhiping and Ju, Wei and Zhang, Ming},
  journal = {arXiv preprint arXiv:2410.14745},
  year    = {2024}
}
@incollection{kamath2024llm,
  title     = {LLM Adaptation and Utilization},
  author    = {Kamath, Uday and Keenan, Kevin and Somers, Garrett and Sorenson, Sarah},
  booktitle = {Large Language Models: A Deep Dive: Bridging Theory and Practice},
  pages     = {135--175},
  year      = {2024},
  publisher = {Springer}
}
@misc{LLM4AD,
  title         = {Large Language Models for Autonomous Driving (LLM4AD): Concept, Benchmark, Experiments, and Challenges},
  author        = {Can Cui and Yunsheng Ma and Zichong Yang and Yupeng Zhou and Peiran Liu and Juanwu Lu and Lingxi Li and Yaobin Chen and Jitesh H. Panchal and Amr Abdelraouf and Rohit Gupta and Kyungtae Han and Ziran Wang},
  year          = {2025},
  eprint        = {2410.15281},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO},
  url           = {https://arxiv.org/abs/2410.15281}
}
@misc{llmSurvey,
  title         = {A Survey of Large Language Models},
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2025},
  eprint        = {2303.18223},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2303.18223}
}

@inproceedings{attentionIsAllYouNeed,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  title     = {Attention is all you need},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6000–6010},
  numpages  = {11},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@software{unrealengine,
  author  = {{Epic Games}},
  title   = {Unreal Engine},
  url     = {https://www.unrealengine.com},
  version = {4.22.1},
  date    = {2019-04-25}
}
@article{lgsvl,
  title   = {LGSVL Simulator: A High Fidelity Simulator for Autonomous Driving},
  author  = {Rong, Guodong and Shin, Byung Hyun and Tabatabaee, Hadi and Lu, Qiang and Lemke, Steve and Mo{\v{z}}eiko, M{\=a}rti{\c{n}}{\v{s}} and Boise, Eric and Uhm, Geehoon and Gerow, Mark and Mehta, Shalin and others},
  journal = {arXiv preprint arXiv:2005.03778},
  year    = {2020}
}
@inproceedings{numba,
  title     = {Numba: A llvm-based python jit compiler},
  author    = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  booktitle = {Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC},
  pages     = {1--6},
  year      = {2015}
}

@article{go,
  title     = {The go programming language},
  author    = {Meyerson, Jeff},
  journal   = {IEEE software},
  volume    = {31},
  number    = {5},
  pages     = {100--104},
  year      = {2014},
  publisher = {IEEE}
}

@misc{goldberg2015primerneuralnetworkmodels,
  title         = {A Primer on Neural Network Models for Natural Language Processing},
  author        = {Yoav Goldberg},
  year          = {2015},
  eprint        = {1510.00726},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1510.00726}
}

% TODO: Burde se over at denne blir riktig i outputen
@book{marsland,
  added-at  = {2014-06-02T22:49:10.000+0200},
  author    = {Marsland, Stephen},
  biburl    = {https://www.bibsonomy.org/bibtex/28808fc9257c5b5216873086c646da37c/hpschu},
  ee        = {http://www.crcpress.com/product/isbn/9781420067187},
  interhash = {11e7f27e7583e5c1e46fc191ab146919},
  intrahash = {8808fc9257c5b5216873086c646da37c},
  isbn      = {978-1-4200-6718-7},
  keywords  = {learning machine},
  pages     = {I-XVI, 1-390},
  publisher = {CRC Press},
  series    = {Chapman and Hall / CRC machine learning and pattern recognition series},
  timestamp = {2014-06-02T22:49:10.000+0200},
  title     = {Machine Learning - An Algorithmic Perspective.},
  edition   = {2nd},
  year      = 2015
}


@article{watson2005autonomous,
  title   = {Autonomous systems},
  author  = {Watson, David P and Scheidt, David H},
  journal = {Johns Hopkins APL technical digest},
  volume  = {26},
  number  = {4},
  pages   = {368--376},
  year    = {2005}
}

@inproceedings{ADTestingReview16,
  title        = {Autonomous vehicles testing methods review},
  author       = {Huang, WuLing and Wang, Kunfeng and Lv, Yisheng and Zhu, FengHua},
  booktitle    = {2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},
  pages        = {163--168},
  year         = {2016},
  organization = {IEEE}
}

@article{safeToDrive,
  title     = {Is it safe to drive? An overview of factors, metrics, and datasets for driveability assessment in autonomous driving},
  author    = {Guo, Junyao and Kurup, Unmesh and Shah, Mohak},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  volume    = {21},
  number    = {8},
  pages     = {3135--3151},
  year      = {2019},
  publisher = {IEEE}
}

@inproceedings{scenes,
  author    = {Ulbrich, Simon and Menzel, Till and Reschka, Andreas and Schuldt, Fabian and Maurer, Markus},
  booktitle = {2015 IEEE 18th International Conference on Intelligent Transportation Systems},
  title     = {Defining and Substantiating the Terms Scene, Situation, and Scenario for Automated Driving},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {982-988},
  keywords  = {Vehicles;Observers;Context modeling;Planning;Vehicle dynamics;Roads;Systems architecture},
  doi       = {10.1109/ITSC.2015.164}
}


@inproceedings{scenarioTysk,
  author    = {Wershofen, Klaus Peter
               and Graefe, Volker},
  editor    = {Schmidt, G{\"u}nther
               and Freyberger, Franz},
  title     = {Situationserkennung als Grundlage der Verhaltenssteuerung eines mobilen Roboters},
  booktitle = {Autonome Mobile Systeme 1996},
  year      = {1996},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {170--179},
  abstract  = {Ein neuartiges Systemkonzept f{\"u}r verhaltensbasierte mobile Roboter, die objektorientierte verhaltensbasierte Navigation, wird vorgestellt. Kernpunkt dabei ist, da{\ss} die Verhaltensauswahl situationsgesteuert erfolgt. Der hierf{\"u}r ma{\ss}gebliche Situationsbegriff wird erl{\"a}utert; er ergibt sich im wesentlichen aus den Zust{\"a}nden der in der Umgebung des Roboters befindlichen k{\"o}rperlichen Objekte und des Roboters selbst. Voraussetzungen f{\"u}r eine Realisierung des Konzepts sind eine leistungsf{\"a}hige Sensorik und eine angepa{\ss}te Wissensrepr{\"a}sentation. Ein globales Koordinatensystem und eine genaue Kenntnis der geometrischen Gegebenheiten des Einsatzgebiets des Roboters sind dagegen nicht erforderlich.},
  isbn      = {978-3-642-80324-6}
}

@inproceedings{schmidtScenario,
  author    = {Schmidt, Max Theo and Hofmann, Ulrich and Bouzouraa, M. Essayed},
  booktitle = {17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  title     = {A novel goal oriented concept for situation representation for ADAS and automated driving},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {886-893},
  keywords  = {Vehicles;Surface acoustic waves;Automation;Communities;Sensors;Uncertainty;Decision making},
  doi       = {10.1109/ITSC.2014.6957801}
}



@inproceedings{autoSceneGen,
  title={Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner},
  author={Aiersilan, Aizierjiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={14},
  pages={14539--14547},
  year={2025},
  doi = {10.1609/aaai.v39i14.33593}
}

@inproceedings{egoDefinition,
  title={Coverage Metrics for a Scenario Database for the Scenario-Based Assessment of Automated Driving Systems},
  author={de Gelder, Erwin and Buermann, Maren and Den Camp, Olaf Op},
  booktitle={2024 IEEE International Automated Vehicle Validation Conference (IAVVC)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@inproceedings{airsim,
  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  year = {2017},
  booktitle = {Field and Service Robotics},
  eprint = {arXiv:1705.05065},
  url = {https://arxiv.org/abs/1705.05065}
}

@misc{emergentabilitiesLLM,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@article{anderson1972more,
  title={More Is Different: Broken symmetry and the nature of the hierarchical structure of science.},
  author={Anderson, Philip W},
  journal={Science},
  volume={177},
  number={4047},
  pages={393--396},
  year={1972},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{parrot,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{sapirWhorf,
title = {Reference in memorial tribute to Eric Lenneberg},
journal = {Cognition},
volume = {4},
number = {2},
pages = {125-153},
year = {1976},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(76)90001-9},
url = {https://www.sciencedirect.com/science/article/pii/0010027776900019},
author = {Roger Brown}
}

@article{conceptBlending,
author = {Fauconnier, Gilles and Turner, Mark},
year = {2003},
month = {03},
pages = {},
title = {Conceptual Blending, Form and Meaning},
volume = {19},
journal = {Recherches en Communication; No 19: Sémiotique cognitive — Cognitive Semiotics; 57-86},
doi = {10.14428/rec.v19i19.48413}
}

@article{prompting,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560815},
doi = {10.1145/3560815},
abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {195},
numpages = {35},
keywords = {Pre-trained language models, prompting}
}


@article{llmCarbon,
  title={The carbon emissions of writing and illustrating are lower for AI than for humans},
  author={Tomlinson, Bill and Black, Rebecca W and Patterson, Donald J and Torrance, Andrew W},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={3732},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@Book{jm,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released January 12, 2025",
  edition =         "3rd",
  }